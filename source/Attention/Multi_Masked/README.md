### Masked_Multi_Head_Attention_Layer
 Change the Computation of the Attention Head
- With the Masked Softmax we will [Compute_masked_attention_heads()](../../Mask/mask.py) for each query by **masking the future tokens** :

### *The only difference with the Multi_Head_Attention
- ##### *adding a mask in parameters* 
- ##### *a different attention calculation*